 
### FLOCK LOAD COMPUTING ###
#--------------------------#

# OLD: compute UD using kernel around points (bivariate normal or Epanechnikov kernel)
flock_load_from_daily_and_state_UD <- function(UD, n_points_state, n_points_total, flock_size, prop_time_collar_on) {
    # INPUTS
    #   UD : the kernel computed by adehabitatHR for this day and this state
    #   n_points_state : number of relocation attributed to this state this day
    #   n_points_total : total number of relocations this day
    #   flock_size : number of animals in the flock
    # OUTPUT
    #   a data.frame with Charge and x and y coordinates

    UD <- as.data.frame.estUD(UD)
    colnames(UD) <- c("Charge", "x", "y")
    UD$Charge <- (UD$Charge
        / sum(UD$Charge)
        * n_points_state
        / n_points_total
        * flock_size
        * prop_time_collar_on
        * 10000 / abs(min(diff(unique(UD$x)))) / abs(min(diff(unique(UD$y))))) # To get a result in animals/ha for each pixel
    return(UD)
}

flock_load_by_day_and_state_to_rds_kernelUD <- function(data, grid, save_dir, save_rds_name, flock_size, prop_time_collar_on) {
    # Compute flock load by day and state
    # INPUTS
    #    data : a dataframe of animal relocations with x, y and time columns
    #    grid : a spatialPixelDataFrame object, the load will be computed on the same grid
    #    save_dir : path to the directory the results will be saved to.
    #    save_rds_name : name of the rds file the results will be saved to.
    #    flock_size : the number of animals in the flock
    # OUPUTS
    #    a rds by state file containing a data.frame with "x", "y", "day" and "Charge" columns 

    data$day <- yday(data$time)
    data <- SpatialPointsDataFrame(data[, c("x", "y")], data)
    proj4string(data) <- CRS_L93

    
    for (state in unique(data$state)) {
        save_rds_prefix = paste0(save_dir, save_rds_name,"_",state,"_")
        if (file.exists(paste0(save_dir,save_rds_name))) {
            file.remove(paste0(save_dir,save_rds_name)) #Delete file if it exists
        }

        days = unique(data$day)

        removed_days =  numeric(0) # kernelUD needs at least 5 relocations, days with less relocations are removed
        for (d in days) {
            if(nrow(data[data$state == state & data$day == d,]) < 5) {
                data = data[!(data$state == state & data$day == d),]
                removed_days = append(removed_days, d)
            }
        }

        hr <- kernelUD(data[data$state == state, "day"], grid = grid, h = h, kern = "epa") # kern :a character string. If "bivnorm", a bivariate normal kernel is used. If "epa", an Epanechnikov kernel is used.
        charge_jour <- flock_load_from_daily_and_state_UD(hr[[as.character(days[!(days %in% removed_days)][1])]], 1, 1, flock_size, prop_time_collar_on)
        ordre = order(charge_jour$x, charge_jour$y) # To get the right x and y orders

        for (d in days) {
            print(paste0("Day ",d,"/",max(unique(data$day))," state ",state))
            if (d %in% removed_days) { # If the day was removed, we consider the load is equal to 0 everywhere
                charge_jour <- pheno_t0 %>%
                                as.data.frame(xy = T) %>%
                                dplyr::select("x", "y") %>%
                                mutate(Charge = 0)
                charge_jour <- charge_jour[order(charge_jour$x, charge_jour$y), ]
            } else {
                charge_jour <- flock_load_from_daily_and_state_UD(hr[[as.character(d)]], 
                                    n_points_state = sum(data$day == d & data$state == state),
                                    n_points_total = sum(data$day == d), flock_size, prop_time_collar_on)
                charge_jour <- charge_jour[ordre, ]
            }
            charge_jour$day <- d
            charge_jour$state <- state
            saveRDS(charge_jour, file = paste0(save_rds_prefix,d,".rds"))
        }
        rm(charge_jour)
        rm(hr)
    }

    # Merge the .rds files in one
    files = list.files(save_dir, pattern=paste0("^",save_rds_name), full.names=T)
    charge = as.data.frame(rbindlist(lapply(files, readRDS), use.names=TRUE))
    saveRDS(charge, paste0(save_dir,save_rds_name))
    lapply(files, file.remove)
}


# NEW: compute UD using a Brownian Bridge kernel around trajectories

flock_load_from_daily_and_state_UD_kernelbb <- function(UD, n_points_state, n_points_total, flock_size, prop_time_collar_on) {
    # INPUTS
    #   UD : the kernel computed by adehabitatHR for this day and this state
    #   n_points_state : number of relocation attributed to this state this day
    #   n_points_total : total number of relocations this day
    #   flock_size : number of animals in the flock
    # OUTPUT
    #   a data.frame with Charge and x and y coordinates

    if (is.list(UD)) {
        UD <- lapply(UD, as.data.frame.estUD) %>%
                     do.call(rbind, .)
    }
    else {
        UD <- as.data.frame.estUD(UD)
    }

    UD %>% rename(Charge=ud, x=Var2, y=Var1) %>%
            group_by(x,y) %>%
            summarise(Charge=sum(Charge, na.rm = T), .groups="drop") %>% # it is necessary to remove NA, because if there is only one point in the burst, the corresponding UD is NaN everywhere
            mutate(Charge = (Charge
                            / sum(Charge)
                            * n_points_state
                            / n_points_total
                            * flock_size
                            * prop_time_collar_on
                            * 10000 / abs(min(diff(unique(x)))) / abs(min(diff(unique(y)))))) %>% # To get a result in animals/ha for each pixel
            return()
}


flock_load_by_day_and_state_to_rds_kernelbb <- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on) {
    # Compute flock load by day and state
    # INPUTS
    #    data : a dataframe of animal relocations with x, y and time columns
    #    grid : a spatialPixelDataFrame object, the load will be computed on the same grid
    #    save_dir : path to the directory the results will be saved to.
    #    save_rds_name : name of the rds file the results will be saved to.
    #    flock_sizes : a 365-long vector containing the number of animals in the flock for each year_day (values for days without GPS relocations are not used)
    # OUPUTS
    #    a rds by state file containing a data.frame with "x", "y", "day" and "Charge" columns 

    data$day <- yday(data$time)
    # data <- SpatialPointsDataFrame(data[, c("x", "y")], data)
    # proj4string(data) <- CRS_L93
    if (file.exists(paste0(save_dir,save_rds_name))) {
        file.remove(paste0(save_dir,save_rds_name)) #Delete file if it exists
    }
    Tmax = 42 #in minutes for split_at_gap
    hmin = 15
    # Ds <- list(Repos = 0.7, #for BRB
    #            Paturage = 3,
    #            Deplacement = 8)
    Ds <- list(Repos = 1.25,
                Paturage = 3,
                Deplacement = 4.5)
    
    for (state in unique(data$state)) {
        save_rds_prefix = paste0(save_dir, save_rds_name,"_",state,"_")

        days = unique(data$day)

        data_state = data %>%
                filter(state == !!state)
        ltr = as.ltraj(xy = data_state[c("x", "y")], date = as.POSIXct(data_state$time), id = data_state$ID)

        # print(paste("DIFFUSION COEFFICIENTS COMPUTED FOR STATE",state))
        # print(liker(ltr, rangesig1 = c(0,10), sig2 = 10))
        # print(BRB.D(ltr, Tmax = Tmax, Lmin = 0, habitat = NULL, activity = NULL))

        # Prepare cluster for parallelised computation of daily utilisation distribution
        clus <- makeCluster(ncores, outfile='') # outfile='' is verbose option
        clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "pheno_t0", "flock_sizes", "prop_time_collar_on"), envir = environment())
        clusterExport(clus, as.list(lsf.str(.GlobalEnv))) # export all manually loaded functions
        clusterCall(clus, function() {
            # For the pipes
            suppressPackageStartupMessages(library(tidyverse)) # includes ggplot2 and dplyr among others
            suppressPackageStartupMessages(library(adehabitatHR))
            options(warn=0)
        })

        parLapply(clus, days,
                    function(d) {
                        print(paste0("Day ",d,"/",max(days)," state ",state))

                        ltr = data_state %>%
                            filter(day == d) %>%
                            mutate(time = as.POSIXct(time))

                        if( nrow(ltr) > 0 ) {
                            ltr = split_at_gap(ltr, max_gap = Tmax) #split the trajectory of each individual into bursts of the specified state (otherwise, every bursts are bridged together when using kernelbb)
                            ltr = as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)

                            hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE,
                                    extent = 0.5, nalpha = 25)

                            # hr <- BRB(ltr, Ds[[state]], Tmax = Tmax, Lmin = 1, hmin = hmin, type="UD",
                            # filtershort=TRUE, grid = 20, b=FALSE, same4all=TRUE, extent=1, tau = NULL,
                            # boundary=NULL)

                            charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                n_points_state = sum(data$day == d & data$state == state),
                                                n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)

                            charge_jour$day <- d
                            charge_jour$state <- state
                            saveRDS(charge_jour, file = paste0(save_rds_prefix,d,".rds"))
                        }
                } )
        stopCluster(clus)
    }

    # Merge the .rds files in one
    files = list.files(save_dir, pattern=paste0("^",save_rds_name), full.names=T)
    charge = as.data.frame(rbindlist(lapply(files, readRDS), use.names=TRUE))
    saveRDS(charge, paste0(save_dir,save_rds_name))
    lapply(files, file.remove)
}


split_at_gap <- function(data, max_gap = 60, shortest_track = 0) {
    #' Courtesy Théo Michelot
    #' Split track at gaps
    #'
    #' @param data Data frame with (at least) columns for "ID" and "time"
    #' @param max_gap Longest allowed gap, in minutes (track will be split at longer gaps)
    #' @param shortest_track Shortest track to keep after splitting, in minutes. Shorter
    #' tracks will be removed from the output data set.
    #'
    #' @return Data frame with identical structure as input, where ID column
    #' has been replaced by new ID for split tracks. Old ID still accessible as
    #' ID_old column

    # Number of tracks
    n_tracks <- length(unique(data$ID))

    # Save old ID and reinitialise ID column
    data$ID_old <- data$ID
    data$ID <- character(nrow(data))

    # Loop over tracks (i.e., over IDs)
    for(i_track in 1:n_tracks) {
        # Indices for this track
        ind_this_track <- which(data$ID_old == unique(data$ID_old)[i_track])
        track_length <- length(ind_this_track)

        # Time intervals in min
        dtimes <- difftime(data$time[ind_this_track[-1]],
                           data$time[ind_this_track[-track_length]],
                           units = "mins")

        # Indices of gaps longer than max_gap
        ind_gap <- c(0, which(dtimes > max_gap), track_length)

        # Create new ID based on split track
        subtrack_ID <- rep(1:(length(ind_gap) - 1), diff(ind_gap)) #la diff permet d’avoir la longueur de chaque subtrack, puis le rep va permettre de répéter l’indice ordinal de cette subtrack autant de fois que cette longueur
        data$ID[ind_this_track] <- paste0(data$ID_old[ind_this_track], "-", subtrack_ID)
    }

    # Only keep sub-tracks longer than some duration
    track_lengths <- sapply(unique(data$ID), function(id) {
        ind <- which(data$ID == id)
        difftime(data$time[ind[length(ind)]], data$time[ind[1]], units = "min")
    })
    ID_keep <- names(track_lengths)[which(track_lengths >= shortest_track)]
    data <- subset(data, ID %in% ID_keep)

    return(data)
}


get_flock_size_through_time <- function(alpage, flock_size_file) {
    # INPUTS
    #   alpage: the name of the pasture of interest
    #   flock_size_file: path to a CSV file containing the evolutions of flock sizes (one line is the new size of the flock from the specified date)
    #                    Must contain les colonnes  "alpage", "date_debut_periode", "taille_totale_troupeau"
    # OUTPUT
    #   a 365-long vector containing the number of animals in the flock for each year_day (values for days without GPS relocations are not used)
    dates_sizes <- read.csv(flock_size_file, header=TRUE, sep=",") %>%
        filter(alpage == !!pasture) %>%
        mutate(yday = yday(as.POSIXct(start_period_date, tz="GMT", format="%d/%m/%Y"))) %>%
        arrange(yday)

    sizes = rep(0, 1, 365)
    for (i in 1:nrow(dates_sizes)) {
        sizes[dates_sizes$yday[i]:365] = dates_sizes$herd_total_size[i]
    }
    
    return(sizes)
}


recompute_daily_flock_load_by_state <- function(charge_d, flock_size_d, prop_time_collar_on) {
    charge_tot_init = sum(charge_d$Charge)
    charge_tot_fin = flock_size_d * 
                        prop_time_collar_on *
                        10000 / abs(min(diff(unique(charge_d$x)))) / abs(min(diff(unique(charge_d$y)))) # To get a result in animals/ha for each pixel
    n_states = length(unique(charge_d$state))

    for(s in unique(charge$state)) {
        I_s = (charge_d$state == s)
        charge_d$Charge[I_s] = charge_d$Charge[I_s] / charge_tot_init * charge_tot_fin
    }
    return(charge_d)
}


flock_load_by_day_and_state_to_rds_kernelbb_test <- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on) {
  data$day <- yday(data$time)
  
  # Supprime le fichier existant s'il y en a un
  output_file <- file.path(save_dir, basename(save_rds_name))
  if (file.exists(output_file)) {
    file.remove(output_file)
  }
  
  Tmax = 42  # Max gap en minutes
  hmin = 15
  Ds <- list(Repos = 1.25, Paturage = 3, Deplacement = 4.5)
  
  for (state in unique(data$state)) {
    print(paste("▶️ Traitement de l'état :", state))
    
    # CORRECTION : Utilisation de basename() pour éviter la duplication du chemin
    save_rds_prefix <- file.path(save_dir, paste0(basename(save_rds_name), "_", state, "_"))
    
    days <- unique(data$day)
    data_state <- data %>% filter(state == !!state)
    
    if (nrow(data_state) < 5) {
      print(paste("⚠️ État", state, "trop peu de données. Passage au suivant."))
      next
    }
    
    print("🔄 Initialisation du cluster parallèle...")
    
    # Ferme tout cluster existant avant d'en créer un nouveau
    if (exists("clus")) {
      stopCluster(clus)
      rm(clus)
      gc()
    }
    
    # Création du cluster
    clus <- makeCluster(ncores, outfile="")
    print(clus)
    
    # Exportation des variables nécessaires
    clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "grid", "flock_sizes", "prop_time_collar_on", "Tmax", "Ds", "hmin"), envir = environment())
    clusterExport(clus, as.list(lsf.str(.GlobalEnv)))  # Export des fonctions définies
    clusterCall(clus, function() {
      suppressPackageStartupMessages(library(tidyverse))
      suppressPackageStartupMessages(library(adehabitatHR))
    })
    
    print("🖥️ Exécution du calcul en parallèle...")
    
    tryCatch({
      parLapply(clus, days, function(d) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          saveRDS(charge_jour, file = paste0(save_rds_prefix, d, ".rds"))
        }
      })
    }, error = function(e) {
      print("❌ Erreur dans parLapply() ! Passage à l'exécution séquentielle.")
      for (d in days) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          saveRDS(charge_jour, file = paste0(save_rds_prefix, d, ".rds"))
        }
      }
    })
    
    stopCluster(clus)
    
    print("📁 Fusion des fichiers de sortie...")
    
    files <- list.files(save_dir, pattern=paste0("^", basename(save_rds_name)), full.names=TRUE)
    if (length(files) > 0) {
      charge <- as.data.frame(rbindlist(lapply(files, readRDS), use.names=TRUE))
      saveRDS(charge, output_file)
      lapply(files, file.remove)
    } else {
      print("⚠️ Aucun fichier à fusionner pour", state)
    }
  }
}


flock_load_by_day_and_state_to_rds_kernelbb_test_2 <- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on) {
  data$day <- yday(data$time)
  
  # Supprime le fichier existant s'il y en a un
  output_file <- file.path(save_dir, basename(save_rds_name))
  if (file.exists(output_file)) {
    print(paste("🗑️ Suppression du fichier existant :", output_file))
    file.remove(output_file)
  }
  
  Tmax = 42  # Max gap en minutes
  hmin = 15
  Ds <- list(Repos = 1.25, Paturage = 3, Deplacement = 4.5)
  
  # Liste pour stocker tous les fichiers temporaires
  all_files <- c()
  
  for (state in unique(data$state)) {
    print(paste("▶️ Traitement de l'état :", state))
    
    # Correction : On évite de supprimer les fichiers des autres états
    save_rds_prefix <- file.path(save_dir, paste0(basename(save_rds_name), "_", state, "_"))
    
    days <- unique(data$day)
    data_state <- data %>% filter(state == !!state)
    
    if (nrow(data_state) < 5) {
      print(paste("⚠️ État", state, "trop peu de données. Passage au suivant."))
      next
    }
    
    print("🔄 Initialisation du cluster parallèle...")
    
    # Ferme tout cluster existant avant d'en créer un nouveau
    if (exists("clus") && inherits(clus, "cluster")) {
      print("🚨 Un cluster existant détecté. Fermeture en cours...")
      stopCluster(clus)
      rm(clus)
      gc()
    }
    
    # Création du cluster
    clus <- makeCluster(ncores, outfile="")
    print("✅ Cluster initialisé.")
    
    # Exportation des variables nécessaires
    clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "grid", "flock_sizes", "prop_time_collar_on", "Tmax", "Ds", "hmin"), envir = environment())
    clusterExport(clus, as.list(lsf.str(.GlobalEnv)))  # Export des fonctions définies
    clusterCall(clus, function() {
      suppressPackageStartupMessages(library(tidyverse))
      suppressPackageStartupMessages(library(adehabitatHR))
    })
    
    print("🖥️ Exécution du calcul en parallèle...")
    
    tryCatch({
      parLapply(clus, days, function(d) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          
          save_file <- paste0(save_rds_prefix, d, ".rds")
          print(paste("💾 Sauvegarde du fichier :", save_file))
          saveRDS(charge_jour, file = save_file)
          
          # Ajoute le fichier à la liste pour fusion finale
          all_files <<- c(all_files, save_file)
        }
      })
    }, error = function(e) {
      print("❌ Erreur dans parLapply() ! Passage à l'exécution séquentielle.")
      for (d in days) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          
          save_file <- paste0(save_rds_prefix, d, ".rds")
          print(paste("💾 Sauvegarde du fichier :", save_file))
          saveRDS(charge_jour, file = save_file)
          
          # Ajoute le fichier à la liste pour fusion finale
          all_files <<- c(all_files, save_file)
        }
      }
    })
    
    if (exists("clus") && inherits(clus, "cluster")) {
      print("📌 Fermeture propre du cluster...")
      stopCluster(clus)
      rm(clus)
      gc()
    }
  }
  
  print("📁 Fusion finale des fichiers de sortie...")
  
  if (length(all_files) == 0) {
    print("⚠️ Aucun fichier valide pour la fusion finale.")
    return()
  }
  
  # Vérifier que les fichiers existent avant la fusion
  all_files_valides <- all_files[file.exists(all_files) & file.size(all_files) > 0]
  
  if (length(all_files_valides) > 0) {
    print(paste("📂 Fusion de", length(all_files_valides), "fichiers en un seul fichier final..."))
    charge_final <- as.data.frame(rbindlist(lapply(all_files_valides, readRDS), use.names=TRUE))
    saveRDS(charge_final, output_file)
    print(paste("✅ Fusion réussie. Fichier final :", output_file))
    
    # Suppression des fichiers temporaires après fusion
    lapply(all_files_valides, file.remove)
  } else {
    print("⚠️ Tous les fichiers intermédiaires sont vides ou introuvables. Aucune fusion possible.")
  }
}











flock_load_by_day_and_state_to_rds_kernelbb_test_10 <- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on, ncores = 4) {
  library(lubridate)
  library(tidyverse)
  library(adehabitatHR)
  library(parallel)
  
  data$day <- yday(data$time)
  
  # Vérification du dossier
  if (!dir.exists(save_dir)) {
    dir.create(save_dir, recursive = TRUE)
  }
  
  Tmax = 42  # Max gap en minutes
  hmin = 15
  Ds <- list(Repos = 1.25, Paturage = 3, Deplacement = 4.5)
  
  all_files <- c()  # Liste des fichiers RDS générés
  
  for (state in unique(data$state)) {
    print(paste("Traitement de l'état :", state))
    
    save_rds_prefix <- file.path(save_dir, paste0(save_rds_name, "_", state, "_"))
    
    days <- unique(data$day)
    data_state <- data %>% filter(state == !!state)
    
    if (nrow(data_state) < 5) {
      print(paste("État", state, "trop peu de données. Passage au suivant."))
      next
    }
    
    
    
    if (exists("clus") && inherits(clus, "cluster")) {
      stopCluster(clus)
      rm(clus)
      gc()
    }
    
    clus <- makeCluster(ncores, outfile="")
    print("Cluster initialisé.")
    
    clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "grid", "flock_sizes", "prop_time_collar_on", "Tmax", "Ds", "hmin"), envir = environment())
    clusterExport(clus, as.list(lsf.str(.GlobalEnv)))
    clusterCall(clus, function() {
      suppressPackageStartupMessages(library(tidyverse))
      suppressPackageStartupMessages(library(adehabitatHR))
    })
    
    
    tryCatch({
      parLapply(clus, days, function(d) {
        print(paste0("Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          
          save_file <- file.path(save_dir, paste0(save_rds_name, "_", state, "_", d, ".rds"))
          print(paste("Sauvegarde du fichier :", save_file))
          saveRDS(charge_jour, file = save_file)
          
          all_files <<- c(all_files, save_file)
        }
      })
    }, error = function(e) {
      print("Erreur dans parLapply() ! Passage à l'exécution séquentielle.")
      for (d in days) {
        print(paste0("Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2=10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(hr,
                                                                     n_points_state = sum(data$day == d & data$state == state),
                                                                     n_points_total = sum(data$day == d), flock_sizes[d], prop_time_collar_on)
          
          charge_jour$day <- d
          charge_jour$state <- state
          
          save_file <- file.path(save_dir, paste0(save_rds_name, "_", state, "_", d, ".rds"))
          print(paste("Sauvegarde du fichier :", save_file))
          saveRDS(charge_jour, file = save_file)
          
          all_files <<- c(all_files, save_file)
        }
      }
    })
    
    if (exists("clus") && inherits(clus, "cluster")) {
      stopCluster(clus)
      rm(clus)
      gc()
    }
  }
  
  print("Fichiers individuels générés !!!")
}

flock_merge_rds_files <- function(save_dir, save_rds_name) {
  library(data.table)
  
  print("Fusion des fichiers...")
  
  final_rds_path <- file.path(save_dir, paste0(save_rds_name, alpage, ".rds"))
  
  # 🔹 Trouver tous les fichiers générés
  all_files <- list.files(save_dir, pattern = paste0("^", save_rds_name, "_"), full.names = TRUE)
  
  if (length(all_files) == 0) {
    print("Aucun fichier à fusionner.")
    return(NULL)
  }
  
  # 🔹 Vérifier que les fichiers existent et ne sont pas vides
  all_files_valides <- all_files[file.exists(all_files) & file.size(all_files) > 0]
  
  if (length(all_files_valides) > 0) {
    print(paste("Fusion de", length(all_files_valides), "fichiers en un seul fichier final..."))
    
    charge_final <- as.data.frame(rbindlist(lapply(all_files_valides, readRDS), use.names = TRUE))
    
    saveRDS(charge_final, final_rds_path)
    
    # 🔹 Suppression des fichiers temporaires après fusion
    lapply(all_files_valides, file.remove)
    
    
    return(final_rds_path)
  } else {
    print("Tous les fichiers intermédiaires sont vides ou introuvables. Aucune fusion possible.")
    return(NULL)
  }
}











































































































flock_merge_rds_files <- function(save_dir, state_daily_rds_prefix) {
   # 📌 Lister tous les fichiers RDS générés
  all_files <- list.files(save_dir, pattern = paste0("^", state_daily_rds_prefix), full.names = TRUE)
  
  # 📌 Vérifier s'il y a des fichiers à fusionner
  if (length(all_files) == 0) {
    print("⚠️ Aucun fichier trouvé pour la fusion.")
    return(NULL)
  }
  
  # 📌 Charger tous les fichiers RDS
  rds_list <- lapply(all_files, function(file) {
    data <- readRDS(file)
    
    # Vérification : Si la colonne state est manquante ou mal définie, on la rajoute
    if (!"state" %in% colnames(data)) {
      print(paste("🚨 Problème détecté : fichier sans colonne 'state'", file))
      # Extraire le nom du fichier pour retrouver l'état (ex: "_Repos_" ou "_Paturage_")
      state_detected <- str_extract(basename(file), "_(Repos|Paturage|Deplacement)_")
      state_detected <- gsub("_", "", state_detected)  # Nettoyage du nom
      data$state <- state_detected
    }
    
    return(data)
  })
  
  # 📌 Fusionner tous les fichiers en une seule data.frame
  charge_final <- as.data.frame(rbindlist(rds_list, use.names = TRUE, fill = TRUE))
  
  # 📌 Vérifier que toutes les valeurs state sont bien présentes
  print("🔍 Vérification des valeurs uniques de la colonne 'state' après fusion :")
  print(unique(charge_final$state))
  
  # 📌 Sauvegarde du fichier final
  output_file <- file.path(save_dir, paste0(state_daily_rds_prefix,alpage, ".rds"))
  saveRDS(charge_final, output_file)
  
  print(paste("✅ Fusion réussie ! Fichier final :", output_file))
  
  return(output_file)
}



flock_load_by_day_and_state_to_rds_kernelbb_test_5<- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on, ncores = 4) {
  # 🔹 Vérification et création du dossier de sauvegarde
  if (!dir.exists(save_dir)) {
    dir.create(save_dir, recursive = TRUE)
  }
  
  # 🔹 Suppression de l'ancien fichier fusionné
  final_rds_path <- file.path(save_dir, paste0(save_rds_name, "_merged.rds"))
  if (file.exists(final_rds_path)) {
    file.remove(final_rds_path)
  }
  
  # 🔹 Ajout du jour dans la data
  data$day <- yday(data$time)
  
  # 🔹 Définition des coefficients de diffusion
  Ds <- list(
    Repos = 1.25,
    Paturage = 3,
    Deplacement = 4.5
  )
  
  Tmax <- 42  # Max gap en minutes pour split_at_gap
  hmin <- 15
  
  # 🔹 Liste des fichiers générés
  rds_files <- c()
  
  for (state in unique(data$state)) {
    print(paste("▶️ Traitement de l'état :", state))
    
    save_rds_prefix <- file.path(save_dir, paste0(save_rds_name, "_", state, "_"))
    days <- unique(data$day)
    data_state <- data %>% filter(state == !!state)
    
    if (nrow(data_state) < 5) {
      print(paste("⚠️ État", state, "trop peu de données. Passage au suivant."))
      next
    }
    
    print("🔄 Initialisation du cluster parallèle...")
    
    # 🔹 Fermeture des clusters existants avant d'en créer un nouveau
    if (exists("clus")) {
      stopCluster(clus)
      rm(clus)
      gc()
    }
    
    # 🔹 Création du cluster
    clus <- makeCluster(ncores, outfile = "")
    print(clus)
    
    # 🔹 Exportation des variables nécessaires
    clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "grid", "flock_sizes", "prop_time_collar_on", "Tmax", "Ds", "hmin"), envir = environment())
    clusterExport(clus, as.list(lsf.str(.GlobalEnv)))  # Export des fonctions définies
    clusterCall(clus, function() {
      suppressPackageStartupMessages(library(tidyverse))
      suppressPackageStartupMessages(library(adehabitatHR))
    })
    
    print("🖥️ Exécution du calcul en parallèle...")
    
    # 🔹 Exécution parallèle avec gestion des erreurs
    results <- tryCatch({
      parLapply(clus, days, function(d) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2 = 10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(
            hr,
            n_points_state = sum(data$day == d & data$state == state),
            n_points_total = sum(data$day == d),
            flock_sizes[d],
            prop_time_collar_on
          )
          
          charge_jour$day <- d
          charge_jour$state <- state
          rds_file <- paste0(save_rds_prefix, d, ".rds")
          saveRDS(charge_jour, file = rds_file)
          
          return(rds_file)
        }
        return(NULL)
      })
    }, error = function(e) {
      print("❌ Erreur dans parLapply() ! Passage à l'exécution séquentielle.")
      
      seq_results <- list()
      for (d in days) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2 = 10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(
            hr,
            n_points_state = sum(data$day == d & data$state == state),
            n_points_total = sum(data$day == d),
            flock_sizes[d],
            prop_time_collar_on
          )
          
          charge_jour$day <- d
          charge_jour$state <- state
          rds_file <- paste0(save_rds_prefix, d, ".rds")
          saveRDS(charge_jour, file = rds_file)
          seq_results <- c(seq_results, rds_file)
        }
      }
      return(seq_results)
    })
    
    stopCluster(clus)
    
    rds_files <- c(rds_files, unlist(results))
  }
  
  # 🔹 Nettoyage de la liste des fichiers (suppression des NULL et fichiers inexistants)
  rds_files <- rds_files[!is.na(rds_files) & file.exists(rds_files)]
  
  # 🔹 Fusion des fichiers `.rds`
  merge_rds_files(rds_files, final_rds_path)
  
  print(paste("🎉 Processus terminé. Fichier final :", final_rds_path))
}

# 🛠 Fonction de fusion des fichiers `.rds`
merge_rds_files <- function(files, output_file) {
  if (length(files) == 0) {
    stop("⚠️ Aucun fichier valide trouvé pour la fusion.")
  }
  
  print("📂 Chargement des fichiers pour fusion...")
  
  all_data <- lapply(files, function(f) {
    if (file.exists(f) && file.size(f) > 0) {
      tryCatch({
        readRDS(f)
      }, error = function(e) {
        print(paste("⚠️ Erreur de lecture pour", f, ":", e$message))
        return(NULL)
      })
    } else {
      print(paste("⚠️ Fichier inexistant ou vide :", f))
      return(NULL)
    }
  })
  
  all_data <- Filter(Negate(is.null), all_data)
  
  if (length(all_data) == 0) {
    stop("❌ Aucun fichier lisible. Vérifie tes fichiers sources !")
  }
  
  merged_data <- as.data.frame(rbindlist(all_data, use.names = TRUE))
  
  print("✅ Fusion terminée. Enregistrement en cours...")
  
  saveRDS(merged_data, file = output_file)
  
  print(paste("🎉 Fichier fusionné enregistré :", output_file))
}







library(lubridate)
library(tidyverse)
library(adehabitatHR)
library(parallel)
library(data.table)

library(lubridate)
library(tidyverse)
library(adehabitatHR)
library(parallel)
library(data.table)

flock_load_by_day_and_state_to_rds_kernelbb_final <- function(data, grid, save_dir, save_rds_name, flock_sizes, prop_time_collar_on, ncores = 4) {
  # 🔹 Vérification du dossier de sauvegarde
  if (!dir.exists(save_dir)) {
    dir.create(save_dir, recursive = TRUE)
  }
  
  # 🔹 Supprime l'ancien fichier fusionné s'il existe
  final_rds_path <- file.path(save_dir, paste0(save_rds_name, "_merged.rds"))
  if (file.exists(final_rds_path)) {
    file.remove(final_rds_path)
  }
  
  # 🔹 Ajout du jour dans la data
  data$day <- yday(data$time)
  
  # 🔹 Définition des coefficients de diffusion
  Ds <- list(
    Repos = 1.25,
    Paturage = 3,
    Deplacement = 4.5
  )
  
  Tmax <- 42  # Max gap en minutes pour split_at_gap
  hmin <- 15
  
  # 🔹 Liste pour stocker tous les fichiers RDS créés
  all_files <- c()
  
  for (state in unique(data$state)) {
    print(paste("▶️ Traitement de l'état :", state))
    
    # **Correction du problème de chemin**
    save_rds_prefix <- file.path(save_dir, paste0(save_rds_name, "_", state, "_"))
    
    days <- unique(data$day)
    data_state <- data %>% filter(state == !!state)
    
    if (nrow(data_state) < 5) {
      print(paste("⚠️ État", state, "trop peu de données. Passage au suivant."))
      next
    }
    
    print("🔄 Initialisation du cluster parallèle...")
    
    # 🔹 Fermeture de tout cluster existant
    if (exists("clus")) {
      stopCluster(clus)
      rm(clus)
      gc()
    }
    
    # 🔹 Création du cluster
    clus <- makeCluster(ncores, outfile = "")
    print("✅ Cluster initialisé.")
    
    # 🔹 Exportation des variables nécessaires
    clusterExport(clus, list("days", "state", "save_rds_prefix", "data", "data_state", "grid", "flock_sizes", "prop_time_collar_on", "Tmax", "Ds", "hmin"), envir = environment())
    clusterExport(clus, as.list(lsf.str(.GlobalEnv)))  # Export des fonctions définies
    clusterCall(clus, function() {
      suppressPackageStartupMessages(library(tidyverse))
      suppressPackageStartupMessages(library(adehabitatHR))
    })
    
    print("🖥️ Exécution du calcul en parallèle...")
    
    # 🔹 Exécution parallèle avec gestion des erreurs
    tryCatch({
      parLapply(clus, days, function(d) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2 = 10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(
            hr,
            n_points_state = sum(data$day == d & data$state == state),
            n_points_total = sum(data$day == d),
            flock_sizes[d],
            prop_time_collar_on
          )
          
          charge_jour$day <- d
          charge_jour$state <- state
          save_file <- file.path(save_dir, paste0(save_rds_name, "_", state, "_", d, ".rds"))
          saveRDS(charge_jour, file = save_file)
          
          all_files <<- c(all_files, save_file)
        }
      })
    }, error = function(e) {
      print("❌ Erreur dans parLapply() ! Passage à l'exécution séquentielle.")
      
      for (d in days) {
        print(paste0("📅 Jour ", d, "/", max(days), " état ", state))
        
        ltr <- data_state %>% filter(day == d) %>% mutate(time = as.POSIXct(time))
        
        if (nrow(ltr) > 0) {
          ltr <- split_at_gap(ltr, max_gap = Tmax) 
          ltr <- as.ltraj(xy = ltr[c("x", "y")], date = ltr$time, id = ltr$ID)
          
          hr <- kernelbb(ltr, sig1 = Ds[[state]], sig2 = 10, grid = grid, same4all = FALSE, byburst = TRUE, extent = 0.5, nalpha = 25)
          
          charge_jour <- flock_load_from_daily_and_state_UD_kernelbb(
            hr,
            n_points_state = sum(data$day == d & data$state == state),
            n_points_total = sum(data$day == d),
            flock_sizes[d],
            prop_time_collar_on
          )
          
          charge_jour$day <- d
          charge_jour$state <- state
          save_file <- file.path(save_dir, paste0(save_rds_name, "_", state, "_", d, ".rds"))
          saveRDS(charge_jour, file = save_file)
          
          all_files <<- c(all_files, save_file)
        }
      }
    })
    
    stopCluster(clus)
  }
  
  # 🔹 Fusion des fichiers `.rds`
  print("📁 Fusion finale des fichiers...")
  
  if (length(all_files) == 0) {
    print("⚠️ Aucun fichier valide pour la fusion finale.")
    return()
  }
  
  # Vérifier que les fichiers existent avant la fusion
  all_files_valides <- all_files[file.exists(all_files) & file.size(all_files) > 0]
  
  if (length(all_files_valides) > 0) {
    print(paste("📂 Fusion de", length(all_files_valides), "fichiers en un seul fichier final..."))
    charge_final <- as.data.frame(rbindlist(lapply(all_files_valides, readRDS), use.names = TRUE))
    saveRDS(charge_final, final_rds_path)
    print(paste("✅ Fusion réussie. Fichier final :", final_rds_path))
    
    # Suppression des fichiers temporaires après fusion
    lapply(all_files_valides, file.remove)
  } else {
    print("⚠️ Tous les fichiers intermédiaires sont vides ou introuvables. Aucune fusion possible.")
  }
}


#Test